\chapter{Обзор методов координации агентов в многоагентных системах}

\section{Классификация методов координации агентов в многоагентных системах}

Классификация методов координации агентов в многоагентных системах (МАС) позволяет структурировать подходы к решению задач, связанных с организацией взаимодействия агентов.
В данной работе классификация методов проводится с учётом особенностей задачи визуального покрытия критических областей, а также требований к моделированию в игровых приложениях. Для описания методов предлагается следующая классификация:

\subsubsection*{1. Тип взаимодействия между агентами}
Методы координации могут быть разделены на централизованные и децентрализованные:
\begin{itemize}
	\item \textbf{Централизованные методы}: предполагают наличие центрального узла (контроллера), который координирует действия всех агентов.
	Такие методы обеспечивают глобальную оптимальность решений, но увеличивают нагрузку на центральный процессор и могут быть менее устойчивыми к сбоям связи;
	\item \textbf{Децентрализованные методы}: каждое агентное решение принимается на основе локальной информации и взаимодействия с соседними агентами.
	Эти методы более устойчивы к сбоям и масштабируемы, но могут приводить к субоптимальным решениям в глобальной перспективе.
\end{itemize}

\subsubsection*{2. Область восприятия агента}
Методы различаются по тому, какую часть среды может учитывать агент при принятии решений:
\begin{itemize}
	\item \textbf{Методы с глобальным восприятием}: агенты имеют доступ к информации обо всей среде, включая местоположение всех критических точек, других агентов и угроз.
	Это требует высокой вычислительной мощности и связности сети.  
	\item \textbf{Методы с локальным восприятием}: решения принимаются на основе информации из ограниченной области вокруг агента.
	Это снижает требования к вычислительным ресурсам, но может увеличить риск неполного охвата критических точек.
\end{itemize}

\subsubsection*{3. Способ распределения задач между агентами}
Эффективность координации зависит от способа распределения задач:
\begin{itemize}
	\item \textbf{Жёстко распределённые задачи}: каждому агенту заранее назначается определённая область или роль, что упрощает координацию, но снижает адаптивность к изменяющимся условиям.
	\item \textbf{Динамическое распределение задач}: задачи перераспределяются в процессе выполнения на основе текущей информации о среде.
	Этот подход обеспечивает большую гибкость, но требует дополнительных вычислений.
\end{itemize}

\subsubsection*{4. Реализационная сложность алгоритма}
Классификация методов должна учитывать их вычислительную сложность:
\begin{itemize}
	\item \textbf{Алгоритмы с полиномиальной сложностью}: подходят для применения в игровых приложениях, где требуется работа в режиме мягкого реального времени.  
	\item \textbf{Алгоритмы с экспоненциальной сложностью}: могут использоваться для моделирования небольших систем или предварительных расчётов, но не пригодны для динамического применения в игре.
\end{itemize}
Анализ сложности должен учитывать как время вычислений, так и объём памяти, необходимый для хранения информации о среде и агентных состояниях.

\subsubsection*{5. Гибкость метода к изменяющимся условиям}
Задача визуального контроля критических областей в играх требует учёта изменяющейся среды и появления новых угроз.
В связи с этим методы могут быть классифицированы следующим образом:
\begin{itemize}
	\item \textbf{Методы с низкой адаптивностью}: эффективно работают в статических средах, но требуют значительного времени для перерасчёта при изменении условий.  
	\item \textbf{Методы с высокой адаптивностью}: автоматически корректируют действия агентов в ответ на изменения в среде, что делает их подходящими для задач в динамических игровых приложениях.
\end{itemize}

\subsubsection*{6. Визуальная правдоподобность движений агентов}
Для игровых приложений важно, чтобы движения агентов выглядели естественно с точки зрения игрока.
Методы могут быть классифицированы по уровню визуальной правдоподобности:
\begin{itemize}
	\item \textbf{Прямолинейные методы}: движения агентов строго следуют оптимальной траектории.
	Это может быть эффективно с точки зрения минимизации затрат, но выглядит механистично и неестественно.  
	\item \textbf{Методы с естественным поведением}: включают элементы непредсказуемости или плавности в траекториях агентов, что улучшает восприятие их действий игроком.
\end{itemize}

\section{Методы координации агентов в многоагентных системах}

В данном разделе рассматриваются основные методы координации агентов, которые могут быть использованы для решения задачи визуального контроля критических областей. Каждый метод кратко описан, а затем классифицирован по критериям, представленным в предыдущем разделе.

\subsection{Метод потенциалов}
Метод потенциалов основывается на вычислении градиента искусственного потенциала, который направляет движение агентов.
Для применения к нашей задаче метод должен учитывать следующие элементы: препятствия, критические области, угрозы, а также общую функцию опасности.

\subsubsection*{Общее описание метода потенциалов}
Потенциал для агента $a_i$ задаётся как функция:
\begin{equation}
	V(p_i) = V_{\text{аттракция}}(p_i) + V_{\text{репеллент}}(p_i),
\end{equation}
где $p_i$ — позиция агента.  
Компоненты потенциала определяются следующим образом:  
\begin{itemize}
	\item $V_{\text{аттракция}}(p_i)$ — компонент, притягивающий агента к целевым областям (например, критическим точкам).
	\item $V_{\text{репеллент}}(p_i)$ — компонент, отталкивающий агента от препятствий, других агентов и областей с высоким уровнем опасности.
\end{itemize}

Градиент потенциала $\nabla V(p_i)$ определяет направление движения агента:
\begin{equation}
	\dot{p}_i = -\nabla V(p_i),
\end{equation}
где $\dot{p}_i$ — скорость агента.

\subsubsection*{Адаптация метода к задаче}
Для нашей задачи потенциал должен учитывать:
\begin{itemize}
	\item Привлечение агентов к критическим точкам и областям с высокой функцией опасности.
	\item Отталкивание агентов от препятствий и других агентов.
	\item Отталкивание агентов от областей с высокой плотностью угроз.
\end{itemize}

Потенциал определяется как:
\begin{equation}
	V(p_i) = \sum_{k=1}^{c} w_k \cdot V_{\text{кр}}(p_i, q_k) + \sum_{j=1}^{b} w_j \cdot V_{\text{преп}}(p_i, o_j) + \sum_{\tau=1}^{t} w_\tau \cdot V_{\text{угр}}(p_i, \tau),
\end{equation}
где:  
\begin{itemize}
	\item $V_{\text{кр}}(p_i, q_k)$ — аттрактор, притягивающий агента $a_i$ к критической точке $q_k$.
	\item $V_{\text{преп}}(p_i, o_j)$ — репеллент, отталкивающий агента $a_i$ от препятствия $o_j$.
	\item $V_{\text{угр}}(p_i, \tau)$ — репеллент, отталкивающий агента $a_i$ от угрозы $\tau$.
	\item $w_k, w_j, w_\tau$ — весовые коэффициенты.
\end{itemize}

Каждая компонента определяется как:
\begin{align}
	V_{\text{кр}}(p_i, q_k) &= -\frac{1}{\|p_i - q_k\| + \epsilon}, \\
	V_{\text{преп}}(p_i, o_j) &= \frac{1}{\|p_i - o_j\|^2 + \epsilon}, \\
	V_{\text{угр}}(p_i, \tau) &= \frac{1}{\|p_i - \tau\|^2 + \epsilon},
\end{align}
где $\epsilon > 0$ предотвращает деление на ноль.

\subsubsection*{Алгоритмическая сложность метода}
Сложность метода определяется количеством вычислений потенциала для каждого агента.  
\begin{itemize}
	\item Для каждой критической точки $q_k$ требуется $O(c)$ вычислений потенциала.
	\item Для каждого препятствия $o_j$ требуется $O(b)$ вычислений.
	\item Для каждой угрозы $\tau$ требуется $O(t)$ вычислений.
\end{itemize}
Итоговая сложность для одного агента:
\begin{equation}
	O_{\text{агент}} = O(c + b + t).
\end{equation}
Суммарная сложность для всех агентов:
\begin{equation}
	O_{\text{общая}} = O(a \cdot (c + b + t)).
\end{equation}

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{децентрализованным}, так как каждый агент принимает решения на основе локальных вычислений потенциала.  

\textbf{Область восприятия:}  
Метод использует \textbf{локальное восприятие}, ограниченное областью действия потенциала.  

\textbf{Распределение задач:}  
Задачи распределяются \textbf{динамически} в процессе вычисления градиента.  

\textbf{Сложность:}  
Итоговая сложность $O(a \cdot (c + b + t))$ является линейной относительно числа агентов $a$ и элементов среды.
Это позволяет применять метод в режиме реального времени.  

\textbf{Гибкость:}  
Метод обладает \textbf{высокой адаптивностью}, так как параметры потенциалов можно изменять в зависимости от текущих условий.  

\textbf{Правдоподобность:}  
Метод обеспечивает \textbf{средний уровень правдоподобности}, так как движение агентов может выглядеть механистично из-за прямолинейного следования градиенту.

\subsection{Метод ролей}

Метод ролей основывается на назначении фиксированных функций или ролей агентам, которые определяют их поведение и задачи в системе.
Для применения к нашей задаче данный метод должен учитывать распределение агентов по функциям патрулирования, защиты критических областей и нейтрализации угроз.

\subsubsection*{Общее описание метода ролей}
В методе ролей каждому агенту $a_i$ назначается роль $r_i$ из множества допустимых ролей $R$:
\begin{equation}
	r_i \in R = \{\text{патрулирование}, \text{защита}, \text{перехват}\}.
\end{equation}

Каждая роль имеет уникальные задачи:
\begin{itemize}
	\item \textbf{Патрулирование:} агент перемещается по маршруту, покрывающему определённую область.
	\item \textbf{Защита:} агент остаётся вблизи критической точки и контролирует угрозы в её окружении.
	\item \textbf{Перехват:} агент направляется к обнаруженной угрозе для её нейтрализации.
\end{itemize}

Назначение ролей может быть статическим (фиксированное распределение) или динамическим (меняется в зависимости от ситуации).
В нашей задаче используется \textbf{динамическое распределение}, где роли пересматриваются в реальном времени на основе состояния среды.

\subsubsection*{Адаптация метода к задаче}
Адаптация метода ролей требует учёта следующих факторов:
\begin{itemize}
	\item Выбор ролей агентов в зависимости от текущего уровня опасности $\mathcal{U}(x, y, t)$, положения критических точек и угроз.
	\item Оптимизация распределения ролей для минимизации времени реакции на угрозы и покрытия областей.
\end{itemize}

Процесс распределения ролей описывается следующим образом:
\begin{equation}
	r_i = \arg \min_{r \in R} C(r, p_i, \mathcal{U}, T),
	\label{eq:role_assignment}
\end{equation}
где $C(r, p_i, \mathcal{U}, T)$ — функция стоимости назначения роли $r$ агенту $a_i$, зависящая от его положения $p_i$, функции опасности $\mathcal{U}$ и текущего набора угроз $T$.

Функция стоимости для каждой роли определяется как:
\begin{align}
	C_{\text{патрулирование}} &= \sum_{(x, y) \in A_i} \mathcal{U}(x, y, t), \\
	C_{\text{защита}} &= \sum_{q_k \in Q} \frac{1}{\|p_i - q_k\| + \epsilon}, \\
	C_{\text{перехват}} &= \min_{\tau \in T} \frac{\|p_i - \tau\|}{v_i},
\end{align}
где:
\begin{itemize}
	\item $A_i$ — область, закреплённая за агентом $a_i$ для патрулирования.
	\item $Q$ — множество критических точек.
	\item $v_i$ — скорость агента $a_i$.
\end{itemize}

\subsubsection*{Алгоритмическая сложность метода}
Рассмотрим сложность распределения ролей.  
\begin{itemize}
	\item Для патрулирования необходимо вычислить сумму значений функции опасности по закреплённой области $A_i$, что требует $O(v)$ операций (по числу вершин графа).
	\item Для защиты требуется рассчитать расстояние до всех критических точек, что требует $O(c)$ операций.
	\item Для перехвата необходимо вычислить расстояние до всех угроз, что требует $O(t)$ операций.
\end{itemize}

Итоговая сложность распределения ролей для одного агента:
\begin{equation}
	O_{\text{агент}} = O(v + c + t).
\end{equation}

Суммарная сложность для всех агентов:
\begin{equation}
	O_{\text{общая}} = O(a \cdot (v + c + t)).
\end{equation}

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{гибридным}, так как роли могут назначаться централизованно (внешним координатором) или децентрализованно (каждым агентом на основе локальных данных).

\textbf{Область восприятия:}  
Метод использует \textbf{глобальное восприятие}, так как распределение ролей требует информации о всей системе.  

\textbf{Распределение задач:}  
Распределение задач является \textbf{динамическим}, так как роли пересматриваются на основе текущего состояния среды.  

\textbf{Сложность:}  
Сложность $O(a \cdot (v + c + t))$ является линейной относительно числа агентов $a$, что позволяет использовать метод в режиме реального времени, если количество вершин $v$, критических точек $c$ и угроз $t$ остаётся умеренным.  

\textbf{Гибкость:}  
Метод обладает \textbf{высокой гибкостью}, так как роли могут адаптироваться к изменениям в среде.  

\textbf{Правдоподобность:}  
Метод обеспечивает \textbf{высокий уровень правдоподобности}, так как распределение ролей позволяет моделировать координированное поведение агентов.

\subsection{Метод роя}

Метод роя основывается на моделировании поведения агентов как коллективного взаимодействия группы, где каждый агент руководствуется правилами взаимодействия с соседями, формируя поведение, схожее с роем насекомых или стаей птиц.
Данный метод обеспечивает децентрализованную координацию агентов за счёт локальных правил.

\subsubsection*{Общее описание метода роя}
Поведение каждого агента $a_i$ определяется рядом локальных правил:  
1. \textbf{Притяжение:} движение к центру масс соседних агентов:  
\begin{equation}
	f_{\text{притяжение}}(p_i) = k_{\text{пр}} \cdot \left( \frac{1}{|N_i|} \sum_{a_j \in N_i} p_j - p_i \right),
\end{equation}
где $N_i$ — множество агентов в радиусе восприятия $r_{\text{воспр}}$, $k_{\text{пр}}$ — коэффициент притяжения.

2. \textbf{Избегание:} отталкивание от слишком близко расположенных агентов:  
\begin{equation}
	f_{\text{избегание}}(p_i) = \sum_{a_j \in N_i^{\text{близ}}} k_{\text{изб}} \cdot \frac{p_i - p_j}{\|p_i - p_j\|^3},
\end{equation}
где $N_i^{\text{близ}}$ — множество агентов в пределах малого радиуса $r_{\text{мин}}$, $k_{\text{изб}}$ — коэффициент избегания.

3. \textbf{Выравнивание:} согласование направления движения:  
\begin{equation}
	f_{\text{выравнивание}}(p_i) = k_{\text{выр}} \cdot \left( \frac{1}{|N_i|} \sum_{a_j \in N_i} \dot{p}_j - \dot{p}_i \right),
\end{equation}
где $k_{\text{выр}}$ — коэффициент выравнивания.

Суммарное движение агента:
\begin{equation}
	\dot{p}_i = f_{\text{притяжение}}(p_i) + f_{\text{избегание}}(p_i) + f_{\text{выравнивание}}(p_i).
\end{equation}

\subsubsection*{Адаптация метода к задаче}
Для нашей задачи метод роя модифицируется следующим образом:  
1. \textbf{Учёт функции опасности $\mathcal{U}(x, y, t)$}: агенты перемещаются в области, где $\mathcal{U}$ имеет высокие значения.
Это обеспечивается добавлением аттрактора:
\begin{equation}
	f_{\text{опасность}}(p_i) = -k_{\text{оп}} \cdot \nabla \mathcal{U}(p_i, t),
\end{equation}
где $k_{\text{оп}}$ — коэффициент чувствительности к опасности.

2. \textbf{Нейтрализация угроз}: агенты в области видимости угроз $\tau$ перенаправляются к ним.
Дополнительное движение определяется как:
\begin{equation}
	f_{\text{угроза}}(p_i) = -k_{\text{угр}} \cdot \sum_{\tau \in T_{\text{вид}}} \frac{p_i - \tau}{\|p_i - \tau\|^3},
\end{equation}
где $T_{\text{вид}}$ — множество угроз в радиусе видимости агента.

3. \textbf{Балансирование покрытия и плотности}: для предотвращения скопления агентов используется штраф за высокую плотность:
\begin{equation}
	f_{\text{разрежение}}(p_i) = k_{\text{разр}} \cdot \left( \frac{1}{|N_i|} - \rho_{\text{целевая}} \right),
\end{equation}
где $\rho_{\text{целевая}}$ — целевая плотность агентов.

Итоговое движение агента:
\begin{equation}
	\dot{p}_i = f_{\text{притяжение}}(p_i) + f_{\text{избегание}}(p_i) + f_{\text{выравнивание}}(p_i) + f_{\text{опасность}}(p_i) + f_{\text{угроза}}(p_i) + f_{\text{разрежение}}(p_i).
\end{equation}

\subsubsection*{Алгоритмическая сложность метода}
Сложность метода роя определяется числом соседей каждого агента
При фиксированном радиусе восприятия $r_{\text{воспр}}$ среднее число соседей $|N_i| \sim O(\rho \cdot \pi r_{\text{воспр}}^2)$, где $\rho$ — плотность агентов.  

\begin{itemize}
	\item На вычисление взаимодействий для одного агента требуется $O(|N_i|)$.
	\item Учёт функции опасности требует $O(v)$ операций для каждого агента, так как $\mathcal{U}$ задаётся на графе.
	\item Для обнаружения угроз в радиусе видимости необходимо $O(t)$.
\end{itemize}

Итоговая сложность для одного агента:
\begin{equation}
	O_{\text{агент}} = O(|N_i| + v + t).
\end{equation}

Суммарная сложность для всех агентов:
\begin{equation}
	O_{\text{общая}} = O(a \cdot (|N_i| + v + t)).
\end{equation}

При фиксированном радиусе восприятия $r_{\text{воспр}}$, сложность $|N_i|$ остаётся практически постоянной, что делает метод подходящим для реального времени.

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{децентрализованным}, так как агенты принимают решения на основе локальной информации.  

\textbf{Область восприятия:}  
Метод использует \textbf{локальное восприятие}, ограниченное радиусом $r_{\text{воспр}}$.  

\textbf{Распределение задач:}  
Задачи распределяются \textbf{динамически} через взаимодействия с функцией опасности и угрозами.  

\textbf{Сложность:}  
Сложность $O(a \cdot (|N_i| + v + t))$ линейна относительно числа агентов $a$ и остаётся практически константной относительно $|N_i|$, что подходит для игр в реальном времени.  

\textbf{Гибкость:}  
Метод обладает \textbf{высокой гибкостью}, так как может адаптироваться к изменяющимся условиям среды.  

\textbf{Правдоподобность:}  
Метод обеспечивает \textbf{высокий уровень правдоподобности}, так как движение агентов напоминает естественное поведение роя или стаи.

\subsection{Метод планирования на основе теории игр}

Метод планирования на основе теории игр предполагает, что агенты взаимодействуют, решая задачи оптимального поведения в многоагентной среде через формирование и решение математической модели игры~\cite{gurevich2005multiagent}.

\subsubsection*{Общее описание метода}
Модель задачи представляется как стратегическая игра $(A, U)$, где:  
\begin{itemize}
	\item $A = \{A_1, A_2, \dots, A_a\}$ — множество агентов;
	\item $S_i$ — множество стратегий $i$-го агента;
	\item $U_i: S_1 \times S_2 \times \dots \times S_a \to \mathbb{R}$ — функция выигрыша (utility) $i$-го агента, зависящая от стратегий всех агентов.
\end{itemize}

В ходе игры каждый агент выбирает стратегию $s_i \in S_i$, стремясь максимизировать свою функцию выигрыша $U_i$.
Решение задачи игры определяется через нахождение равновесий, например, равновесия Нэша, которые удовлетворяют условию:  
\begin{equation}
	U_i(s_{-i}^*, s_i^*) \geq U_i(s_{-i}^*, s_i) \quad \forall s_i \in S_i,
\end{equation}
где $s_{-i}^*$ — стратегии всех агентов, кроме $i$, в равновесии.

\subsubsection*{Адаптация метода к задаче}
Для нашей задачи метод планирования на основе теории игр модифицируется следующим образом:  

1. \textbf{Множество стратегий}:  
Каждый агент выбирает маршрут и целевую область покрытия.
Множество стратегий $S_i$ для агента $i$ включает все возможные пути вдоль графа среды, ведущие к областям покрытия.  

2. \textbf{Функция выигрыша}:  
Функция выигрыша $U_i$ определяется следующими факторами:
\begin{equation}
	U_i(s_i, s_{-i}) = - \alpha \mathcal{U}(p_i) - \beta \sum_{c_k \in C} \mathcal{U}(c_k) + \gamma \sum_{\tau \in T} d(p_i, \tau),
\end{equation}
где:
\begin{itemize}
	\item $\mathcal{U}(p_i)$ — значение функции опасности в текущем положении $p_i$ агента;
	\item $\mathcal{U}(c_k)$ — значение функции опасности в критической области $c_k$;
	\item $d(p_i, \tau)$ — расстояние до ближайшей угрозы $\tau$;
	\item $\alpha, \beta, \gamma$ — веса, задающие приоритеты поведения.
\end{itemize}

3. \textbf{Решение игры}:  
Игра решается в реальном времени через итеративное приближение равновесия Нэша.
Для этого каждый агент оптимизирует свою стратегию $s_i$, исходя из стратегий остальных агентов $s_{-i}$:
\begin{equation}
	s_i^* = \arg\max_{s_i \in S_i} U_i(s_i, s_{-i}^*).
\end{equation}

4. \textbf{Нейтрализация угроз}:  
Если угроза $\tau$ находится в области видимости агента $i$, стратегия агента автоматически переходит к её преследованию и нейтрализации:
\begin{equation}
	s_i = \arg\min_{s_i \in S_i} d(p_i, \tau).
\end{equation}

\subsubsection*{Алгоритмическая сложность метода}
Сложность метода определяется числом агентов, стратегий и итераций поиска равновесия:
\begin{itemize}
	\item Для каждого агента построение множества стратегий $S_i$ требует $O(v + e)$, так как пути задаются графом среды.
	\item Оценка функции выигрыша $U_i$ для всех стратегий $S_i$ требует $O(|S_i|)$.
	\item Поиск равновесия итеративным методом (например, методом наивной оптимизации) требует $O(k \cdot a \cdot |S_i|)$, где $k$ — число итераций до сходимости.
\end{itemize}
Итоговая сложность:
\begin{equation}
	O_{\text{общая}} = O(k \cdot a \cdot (v + e + |S_i|)).
\end{equation}

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод относится к \textbf{гибридному типу}, так как может быть как кооперативным (оптимизация глобальной цели), так и некооперативным (оптимизация индивидуальных целей).

\textbf{Область восприятия:}  
Метод использует \textbf{глобальное восприятие}, так как решение игры требует информации о стратегиях всех агентов.

\textbf{Распределение задач:}  
Задачи распределяются \textbf{статически}, поскольку стратегия выбирается на основе заранее рассчитанных оптимумов.

\textbf{Сложность:}  
Метод имеет сложность $O(k \cdot a \cdot (v + e + |S_i|))$, что делает его трудоёмким для больших систем с высоким числом агентов.

\textbf{Гибкость:}  
Гибкость метода \textbf{средняя}, так как стратегия агента может адаптироваться только после пересчёта равновесия.

\textbf{Правдоподобность:}  
Правдоподобность \textbf{высокая}, так как поведение агентов, основанное на игровых стратегиях, соответствует ожиданиям от разумной координации.

\subsection{Метод на основе обучения с подкреплением}

\subsubsection*{Общее описание метода}
Метод обучения с подкреплением (RL) моделирует поведение агентов как процесс последовательного принятия решений в среде.
Среда представляется в виде марковского процесса принятия решений (Markov Decision Process, MDP), который задаётся пятёркой $(S, A, P, R, \gamma)$:
\begin{itemize}
	\item $S$ — множество состояний среды,
	\item $A$ — множество возможных действий агента,
	\item $P(s' | s, a)$ — функция переходов между состояниями при выполнении действия $a$,
	\item $R(s, a)$ — функция вознаграждения за выполнение действия $a$ в состоянии $s$,
	\item $\gamma \in [0, 1]$ — коэффициент дисконтирования будущих вознаграждений.
\end{itemize}

Цель агента заключается в максимизации ожидаемой суммарной дисконтированной награды:
\begin{equation}
	G_t = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k}) \right].
\end{equation}

Агент учится стратегии $\pi(a|s)$, которая задаёт вероятность выбора действия $a$ в состоянии $s$.
Обучение стратегии происходит на основе значений функции полезности:
\begin{equation}
	Q^\pi(s, a) = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k}) \bigg| s_t = s, a_t = a \right].
\end{equation}

Классическими алгоритмами обучения являются Q-Learning и глубокое Q-обучение (Deep Q-Learning).

\subsubsection*{Адаптация метода к задаче}
Для задачи визуального покрытия критических областей вводится специфическая структура состояния, действий, и функции награды:
\begin{itemize}
	\item \textbf{Состояния ($S$):} Состояние агента включает:
	\begin{itemize}
		\item Текущую позицию $p_i$ и скорость $\dot{p}_i$ агента,
		\item Значения функции опасности $\mathcal{U}(x, y, t)$ в окрестности агента,
		\item Расположение ближайших критических точек и угроз.
	\end{itemize}
	\item \textbf{Действия ($A$):} Агенты могут выбирать движение в одном из $k$ направлений, задаваемых дискретизацией пространства, или оставаться на месте.
	\item \textbf{Функция вознаграждения ($R$):} Вознаграждение определяется следующим образом:
	\begin{equation}
		R(s, a) = -\alpha \cdot \mathcal{U}(p_i, t) + \beta \cdot I_{\text{угроза\_нейтрализована}} - \gamma \cdot \mathcal{L}(p_i),
	\end{equation}
	где $\mathcal{L}(p_i)$ — штраф за выход за границы области, $\alpha, \beta, \gamma$ — коэффициенты весов, $I_{\text{угроза\_нейтрализована}}$ — индикатор нейтрализации угрозы.
\end{itemize}

Для обучения стратегии используется симуляция среды: агенты взаимодействуют с функцией опасности $\mathcal{U}(x, y, t)$, перемещаются между вершинами графа и реагируют на появление угроз $\tau$.
Модель среды обновляется согласно описанной динамике.

\subsubsection*{Алгоритмическая сложность метода}
Обучение с подкреплением включает два основных этапа:

1. \textbf{Симуляция среды:} При фиксированном числе агентов $a$ симуляция одного шага занимает $O(a \cdot v)$ операций, так как необходимо обновить функцию $\mathcal{U}$ и обработать поведение каждого агента.
2. \textbf{Обновление стратегии:} Для алгоритма Q-Learning требуется обновление таблицы $Q(s, a)$, что занимает $O(|S| \cdot |A|)$ операций.
При использовании Deep Q-Learning сложность определяется числом параметров нейронной сети $n_{\text{параметры}}$.

Итоговая сложность обучения:
\begin{equation}
	O_{\text{обучение}} = O(N \cdot (a \cdot v + |S| \cdot |A|)),
\end{equation}
где $N$ — число шагов симуляции.
Для Deep Q-Learning $|S| \cdot |A|$ заменяется на $n_{\text{параметры}}$.

Применение обученной стратегии в реальном времени требует $O(a \cdot n_{\text{параметры}})$ операций на каждом шаге.

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{гибридным}: агенты обучаются индивидуально, но стратегия может учитывать глобальные факторы через функцию состояния.  

\textbf{Область восприятия:}  
Метод использует \textbf{глобальное восприятие}, так как состояние может включать информацию о всей среде, например, через значения $\mathcal{U}$ в крупных областях.  

\textbf{Распределение задач:}  
Распределение задач \textbf{статическое} во время выполнения, так как стратегия фиксируется после этапа обучения.  

\textbf{Сложность:}  
Обучение требует больших вычислительных ресурсов ($O_{\text{обучение}}$), однако применение стратегии возможно в реальном времени.  

\textbf{Гибкость:}  
Метод обладает \textbf{высокой гибкостью}, так как стратегия может адаптироваться к сложным динамическим сценариям.  

\textbf{Правдоподобность:}  
Метод обеспечивает \textbf{высокую правдоподобность}, так как обученные стратегии могут воспроизводить реалистичное поведение.