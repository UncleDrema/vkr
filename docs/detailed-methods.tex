\iffalse
\section{Метод потенциальных полей}

Метод потенциальных полей представляет собой подход к координации движения агентов, в котором используются искусственные "поля" для управления траекторией. В основе метода лежит идея генерации виртуальных сил, которые направляют агента к целевой точке и одновременно помогают избегать препятствий. Этот подход получил широкое применение в робототехнике и системах управления многими агентами благодаря своей концептуальной простоте и возможности гибкой адаптации к разным задачам.

Пусть пространство, в котором движется агент, представлено как множество $\mathcal{X} \subset \mathbb{R}^n$, где $n$ — размерность пространства. Метод потенциальных полей основывается на построении скалярной функции $U(\mathbf{x})$, называемой потенциалом, для каждой точки $\mathbf{x} \in \mathcal{X}$. Эта функция складывается из двух компонентов:

\begin{equation}
	U(\mathbf{x}) = U_{attr}(\mathbf{x}) + U_{rep}(\mathbf{x}),
\end{equation}

где $U_{attr}(\mathbf{x})$ — притягивающий потенциал, определяющий движение агента к цели, а $U_{rep}(\mathbf{x})$ — отталкивающий потенциал, предотвращающий столкновение с препятствиями.

\textbf{Притягивающий потенциал} $U_{attr}(\mathbf{x})$ определяется как квадратичная функция расстояния до целевой точки $\mathbf{x}_g$:
\begin{equation}
	U_{attr}(\mathbf{x}) = \frac{1}{2}k_{attr}\|\mathbf{x} - \mathbf{x}_g\|^2,
\end{equation}
где $k_{attr}$ — коэффициент притяжения.

\textbf{Отталкивающий потенциал} $U_{rep}(\mathbf{x})$ имеет вид:
\begin{equation}
	U_{rep}(\mathbf{x}) = \begin{cases} 
		\frac{1}{2}k_{rep}\left(\frac{1}{d(\mathbf{x})} - \frac{1}{d_0}\right)^2, & d(\mathbf{x}) \leq d_0, \\
		0, & d(\mathbf{x}) > d_0,
	\end{cases}
\end{equation}
где $d(\mathbf{x})$ — расстояние до ближайшего препятствия, $d_0$ — радиус действия отталкивающей силы, а $k_{rep}$ — коэффициент отталкивания.

\textbf{Результирующая сила} $\mathbf{F}(\mathbf{x})$, действующая на агента, равна градиенту потенциала с обратным знаком:
\begin{equation}
	\mathbf{F}(\mathbf{x}) = -\nabla U(\mathbf{x}) = \mathbf{F}_{attr}(\mathbf{x}) + \mathbf{F}_{rep}(\mathbf{x}),
\end{equation}
где
\begin{align}
	\mathbf{F}_{attr}(\mathbf{x}) &= -\nabla U_{attr}(\mathbf{x}) = -k_{attr}(\mathbf{x} - \mathbf{x}_g), \\
	\mathbf{F}_{rep}(\mathbf{x}) &= -\nabla U_{rep}(\mathbf{x}).
\end{align}

Метод потенциальных полей предполагает следующую последовательность действий:
\begin{enumerate}
	\item Построение потенциала $U(\mathbf{x})$ для рабочей области. Это включает определение целевой точки $\mathbf{x}_g$ и всех препятствий.
	\item Вычисление результирующей силы $\mathbf{F}(\mathbf{x})$ для текущей позиции агента.
	\item Обновление положения агента согласно уравнению движения:
	\begin{equation}
		\mathbf{x}_{t+1} = \mathbf{x}_t + \Delta t \cdot \mathbf{F}(\mathbf{x}_t),
	\end{equation}
	где $\Delta t$ — шаг по времени.
\end{enumerate}
Процесс продолжается до тех пор, пока агент не достигнет целевой точки $\mathbf{x}_g$ или пока не произойдет прерывание из-за невозможности продолжать движение.

Метод потенциальных полей децентрализован: каждый агент использует локальную информацию о цели и ближайших препятствиях. Это упрощает реализацию и снижает требования к обмену данными между агентами. Однако метод обладает ограниченной устойчивостью к динамическим изменениям среды. Он хорошо работает в статических условиях, однако может сталкиваться с проблемами в высокодинамичных средах, например, при быстром появлении новых препятствий.

Алгоритм вычисления сил и обновления положения агента требует минимальных затрат, что делает метод подходящим для систем с ограниченными ресурсами. Благодаря децентрализованной природе метод легко масштабируется на группы агентов, не требуя сложной коммуникации между ними. Однако метод ограничен в задачах, где требуется равномерное покрытие области, так как его основная цель — достижение целевой точки, а не равномерное распределение.

Основной недостаток метода — вероятность попадания в локальные минимумы, где результирующая сила становится равной нулю, что препятствует дальнейшему движению. Для улучшения метода в таких условиях можно использовать его комбинацию с другими подходами, например, многослойными потенциальными полями или алгоритмами избегания локальных минимумов.

Рассмотрим задачу перемещения группы агентов в игровой среде для визуального контроля критических областей. Пусть область представляет собой плоское пространство с несколькими препятствиями и одной целевой зоной. Применение метода потенциальных полей позволит эффективно направлять агентов к целевой зоне, избегая столкновений, при условии, что препятствия имеют простую структуру.

Метод потенциальных полей подходит для задач, где требуется простая навигация и минимизация вычислительных затрат. Однако его ограниченная устойчивость к локальным минимумам и сложным динамическим средам делает его менее подходящим для задач, требующих высокой адаптивности. Для улучшения метода в таких условиях можно использовать его комбинацию с другими подходами, такими как многослойные потенциальные поля или алгоритмы глобальной оптимизации.

\section{Метод совместного использования потенциальных полей}

Метод совместного использования потенциальных полей представляет собой модификацию базового метода потенциальных полей. Его основная идея заключается в обмене информацией о локальных потенциальных полях между агентами. Это позволяет улучшить общую координацию и более эффективно избегать локальных минимумов за счёт использования данных от нескольких агентов.

\subsection{Принципы работы}

Каждый агент \(i\) вычисляет своё локальное потенциальное поле \(U_i(\mathbf{x})\), которое состоит из притягивающего компонента \(U_{i,attr}(\mathbf{x})\) и отталкивающего компонента \(U_{i,rep}(\mathbf{x})\):
\[
U_i(\mathbf{x}) = U_{i,attr}(\mathbf{x}) + \sum_{j=1}^k U_{i,rep}^j(\mathbf{x}),
\]
где \(k\) — количество препятствий в пространстве \( \mathcal{X} \), а \(U_{i,rep}^j(\mathbf{x})\) — отталкивающее поле, связанное с препятствием \(O_j\).

При использовании метода совместного использования потенциальных полей агенты передают информацию о своих локальных полях своим соседям. В результате каждый агент формирует глобальное потенциальное поле \(U_{global}(\mathbf{x})\), интегрируя данные от соседних агентов:
\[
U_{global}(\mathbf{x}) = \sum_{i=1}^N w_i U_i(\mathbf{x}),
\]
где \(N\) — общее количество агентов, а \(w_i\) — весовой коэффициент, отражающий важность информации от агента \(i\). Этот подход позволяет учитывать влияние каждого агента на глобальное поведение системы.

Силовое воздействие на каждого агента определяется градиентом глобального поля:
\[
\mathbf{F}_{global}(\mathbf{x}) = -\nabla U_{global}(\mathbf{x}).
\]
Таким образом, движение агента учитывает как притяжение к цели, так и избегание препятствий, с поправкой на данные, полученные от соседних агентов.

\subsection{Характеристики метода}

Метод совместного использования потенциальных полей обладает следующими ключевыми свойствами:

\begin{itemize}
	\item \textbf{Улучшенная координация.} Обмен данными позволяет агентам учитывать действия друг друга, что способствует более скоординированному движению.
	\item \textbf{Снижение вероятности локальных минимумов.} Интеграция данных от соседей помогает агентам избежать застревания в локальных минимумах.
	\item \textbf{Устойчивость к динамическим изменениям среды.} Благодаря обмену информацией агенты могут быстрее адаптироваться к появлению новых препятствий или изменению конфигурации среды.
	\item \textbf{Высокая вычислительная нагрузка.} Обмен данными увеличивает потребности в вычислениях и коммуникации, что может стать ограничением для систем с ограниченными ресурсами.
	\item \textbf{Чувствительность к отказам.} Если один из агентов перестаёт передавать информацию, это может негативно сказаться на общей координации.
\end{itemize}

\subsection{Применение к задачам визуального контроля критических областей}

Метод совместного использования потенциальных полей особенно хорошо подходит для задач, требующих высокой степени координации между агентами. Например, в задаче визуального контроля критических областей, где группа агентов должна патрулировать пространство и оперативно реагировать на угрозы, обмен данными о потенциальных полях позволяет:

\begin{itemize}
	\item Распределить агентов таким образом, чтобы они не пересекались в своих маршрутах.
	\item Избегать столкновений в узких пространствах.
	\item Увеличить плотность покрытия за счёт равномерного распределения агентов по рабочей области.
\end{itemize}

Однако следует учитывать, что метод может оказаться неэффективным в условиях ограниченной пропускной способности сети. Также он требует наличия средств для управления задержками передачи данных, чтобы избежать рассинхронизации между агентами.

\subsection{Сравнение с базовым методом}

Основное преимущество метода совместного использования потенциальных полей перед базовым методом заключается в снижении вероятности локальных минимумов и увеличении уровня координации. Это делает его более подходящим для сложных динамических сред с большим числом агентов и препятствий.

Однако базовый метод более эффективен в условиях ограниченных вычислительных и коммуникационных ресурсов, так как он не требует обмена данными. Таким образом, выбор метода зависит от требований задачи: если требуется высокая степень координации и адаптивности, метод совместного использования предпочтителен. Если же важны низкие затраты на вычисления, лучше использовать базовый метод.

\section{Эргодическое покрытие с использованием потенциальных полей}

Эргодическое покрытие — это метод, основанный на теории эргодичности, который используется для равномерного распределения агентов в заданной области. В контексте потенциальных полей данный метод обеспечивает распределение агентов таким образом, чтобы плотность их покрытия соответствовала заданной вероятностной плотности критических областей. Такой подход позволяет эффективно решать задачи координации агентов в многоагентной системе, особенно в условиях, требующих равномерного наблюдения.

\subsection{Принципы работы метода}

Метод эргодического покрытия основывается на генерации потенциальных полей с использованием решений уравнения теплопроводности. Заданное пространство представляется в виде области \( \mathcal{X} \subset \mathbb{R}^n \), на которой определяется целевая плотность покрытия \( \phi(\mathbf{x}) \). Для обеспечения эргодичности движения агентов решается задача минимизации отклонения распределения агентов от функции \( \phi(\mathbf{x}) \). 

Общая формулировка задачи может быть записана как:
\[
\mathcal{E} = \int_{\mathcal{X}} \left( c(\mathbf{x}, t) - \phi(\mathbf{x}) \right)^2 d\mathbf{x},
\]
где \( c(\mathbf{x}, t) \) — временное распределение агентов в области \( \mathcal{X} \), зависящее от их движения. Потенциальные поля, создаваемые для управления агентами, обеспечивают минимизацию функционала \( \mathcal{E} \).

Движение каждого агента управляется силой \( \mathbf{F}(\mathbf{x}) \), вычисляемой на основе градиента потенциального поля:
\[
\mathbf{F}(\mathbf{x}) = -\nabla U(\mathbf{x}),
\]
где \( U(\mathbf{x}) \) представляет собой результирующее потенциальное поле, включающее притягивающие и отталкивающие компоненты. Притягивающие силы направляют агентов к областям с высокой плотностью \( \phi(\mathbf{x}) \), а отталкивающие силы предотвращают столкновения между агентами.

\subsection{Математическое описание}

Для достижения равномерного покрытия применяется решение уравнения теплопроводности:
\[
\frac{\partial u}{\partial t} = \nabla \cdot (D \nabla u) - \lambda (u - \phi),
\]
где \( u \) — временная плотность покрытия, \( D \) — коэффициент диффузии, а \( \lambda \) — параметр, регулирующий соответствие \( u \) и \( \phi \). Значения \( u \) обновляются для каждого агента, что позволяет динамически корректировать их траектории.

\subsection{Преимущества метода}

\begin{itemize}
	\item \textbf{Высокая плотность покрытия.} Метод обеспечивает равномерное распределение агентов в области, соответствующее заданной плотности \( \phi(\mathbf{x}) \).
	\item \textbf{Интегрированное избегание столкновений.} Использование отталкивающих компонентов потенциального поля предотвращает пересечения траекторий агентов.
	\item \textbf{Гибкость.} Метод позволяет адаптироваться к изменениям области наблюдения и целевых точек.
\end{itemize}

\subsection{Недостатки метода}

\begin{itemize}
	\item \textbf{Высокие вычислительные требования.} Расчёт потенциалов и управление движением агентов требуют значительных ресурсов.
	\item \textbf{Сложность реализации.} Интеграция решения уравнений теплопроводности в реальном времени может быть сложной задачей.
\end{itemize}

\subsection{Применение к задачам визуального контроля критических областей}

Эргодическое покрытие идеально подходит для задач, где требуется равномерное наблюдение за областями с различной степенью критичности. Например, в игровой среде, где критические области имеют разную вероятность появления угроз, данный метод позволяет распределять агентов пропорционально этой вероятности. 

Метод также полезен в ситуациях с высокой динамикой, где требуется перераспределение агентов в зависимости от новых данных. Однако для реального времени его применение может быть ограничено из-за вычислительной нагрузки.

\subsection{Сравнение с предыдущими методами}

В отличие от базового метода потенциальных полей и метода совместного использования, эргодическое покрытие фокусируется на равномерном распределении агентов, что делает его более подходящим для задач наблюдения. Однако его вычислительные требования выше, что может ограничить его применение в системах с ограниченными ресурсами.

\section{Метод обучения с подкреплением}

\subsection{Общее описание метода}

Обучение с подкреплением (RL) является одной из наиболее изученных и применяемых техник для координации агентов в многоагентных системах. Оно базируется на концепции взаимодействия агента с окружающей средой через систему вознаграждений и наказаний. Основная цель агента — максимизировать долгосрочную награду, выбирая последовательности действий, которые ведут к выполнению поставленной задачи.

Агент наблюдает за состоянием среды \( S \), выбирает действие \( A \), затем получает новую информацию о среде \( S' \) и вознаграждение \( R \). Такой процесс моделируется как задача Марковского процесса принятия решений (Markov Decision Process, MDP) с пятью основными компонентами:
\begin{itemize}
	\item Множество состояний \( S \).
	\item Множество действий \( A \).
	\item Функция перехода вероятностей \( P(S'|S, A) \).
	\item Функция вознаграждения \( R(S, A) \).
	\item Дисконтный фактор \( \gamma \), определяющий значимость будущих наград.
\end{itemize}

\subsection{Алгоритм реализации}

Алгоритм обучения с подкреплением включает следующие шаги:
\begin{enumerate}
	\item Инициализация агента с начальным состоянием.
	\item Наблюдение за состоянием среды.
	\item Выбор действия на основе текущей политики \( \pi \), которая может быть детерминированной или стохастической.
	\item Выполнение действия и получение вознаграждения от среды.
	\item Обновление политики \( \pi \) на основе нового состояния и награды.
	\item Повторение процесса до достижения критерия сходимости.
\end{enumerate}

Для визуализации процесса обучения можно использовать следующую блок-схему:

\begin{center}
	\text{uml diagram}
\end{center}

\subsection{Преимущества метода}

\begin{itemize}
	\item \textbf{Адаптивность.} Способность агента подстраиваться под изменяющуюся среду.
	\item \textbf{Обучение в условиях неопределённости.} RL работает в ситуациях, где модель среды неизвестна.
	\item \textbf{Масштабируемость.} Возможность применения к большим системам с множеством агентов.
\end{itemize}

\subsection{Недостатки метода}

\begin{itemize}
	\item \textbf{Высокая вычислительная сложность.} Обучение требует значительных вычислительных ресурсов.
	\item \textbf{Медленное сходимость.} RL может требовать большого количества итераций для достижения приемлемого результата.
	\item \textbf{Проблема совместимости.} Агенты могут находить стратегии, противоречащие общей цели системы.
\end{itemize}

\subsection{Применение к задачам координации агентов}

В задачах визуального контроля критических областей RL позволяет агентам изучать оптимальные траектории и стратегии покрытия областей на основе информации о плотности критических областей. Однако успешное применение метода зависит от выбора модели вознаграждения. Например, вознаграждение может быть связано с количеством обнаруженных объектов или плотностью покрытия заданной области.

Метод RL особенно полезен в динамических средах, где требуется адаптация к изменениям, однако его сложность может стать серьёзным ограничением в условиях реального времени.

\section{Метод распределённого планирования (Distributed Planning)}

\subsection{Общее описание метода}

Метод распределённого планирования (Distributed Planning) является одним из фундаментальных подходов к координации агентов в многоагентных системах. Его основная идея заключается в том, что каждый агент разрабатывает собственный план действий, который синхронизируется или координируется с планами других агентов, чтобы достичь общей цели системы.

Распределённое планирование особенно эффективно в системах, где:
\begin{itemize}
	\item агенты обладают ограниченным локальным знанием о среде;
	\item взаимодействие между агентами осуществляется через прямую или косвенную коммуникацию;
	\item необходима децентрализация для повышения отказоустойчивости и масштабируемости.
\end{itemize}

Основные этапы распределённого планирования включают:
\begin{itemize}
	\item Постановку общей задачи.
	\item Разделение общей задачи на подзадачи, распределяемые между агентами.
	\item Построение индивидуальных планов на основе локального знания и заданных ограничений.
	\item Координацию и синхронизацию действий агентов через обмен сообщениями или другим способом.
\end{itemize}

\subsection{Алгоритм реализации}

Алгоритм распределённого планирования включает следующие шаги:
\begin{enumerate}
	\item Определение общей цели системы.
	\item Разделение задачи между агентами на основе их ролей и возможностей.
	\item Локальная разработка плана каждым агентом.
	\item Обмен сообщениями между агентами для синхронизации планов.
	\item Исполнение действий и корректировка планов в случае необходимости.
\end{enumerate}

Для представления процесса используется следующая блок-схема:

\begin{center}
	\text{plantuml}
\end{center}

\subsection{Преимущества метода}

\begin{itemize}
	\item \textbf{Децентрализация.} Снижает зависимость от центрального контроллера, улучшая отказоустойчивость системы.
	\item \textbf{Масштабируемость.} Позволяет эффективно управлять системами с большим количеством агентов.
	\item \textbf{Адаптивность.} Агенты могут обновлять свои планы в ответ на изменения в среде или системе.
\end{itemize}

\subsection{Недостатки метода}

\begin{itemize}
	\item \textbf{Коммуникационные затраты.} Координация между агентами требует обмена значительным количеством сообщений.
	\item \textbf{Конфликты планов.} Возможно возникновение конфликтов между индивидуальными планами агентов.
	\item \textbf{Ограничения локального знания.} Решения агентов могут быть не оптимальными из-за отсутствия полной информации о среде.
\end{itemize}

\subsection{Применение к задачам координации агентов}

Метод распределённого планирования широко используется в системах управления роботами, интеллектуальном транспорте и автоматизации производства. В задаче визуального контроля критических областей он позволяет агентам эффективно распределять области для обзора, минимизируя перекрытия и улучшая покрытие. Например, агенты могут заранее согласовать, какие области они будут патрулировать, а затем корректировать свои планы в ответ на изменения в среде (например, появление новых критических областей).

\section{Метод координации на основе сигналов (Signal-Based Coordination)}

\subsection{Общее описание метода}

Метод координации на основе сигналов заключается в использовании сигналов, передаваемых между агентами, для управления их действиями и взаимодействиями.  
Сигналы могут быть как физическими (например, световые или звуковые импульсы), так и цифровыми (обмен данными в сетях).  
Основной принцип метода заключается в том, что агенты реагируют на поступающие сигналы и адаптируют свои действия, чтобы достичь общей цели.

Сигналы могут содержать информацию о текущем состоянии агентов, их задачах или окружающей среде.  
Метод часто используется в системах, где коммуникация между агентами ограничена, а также в децентрализованных системах, где отсутствует центральный координатор.

\subsection{Алгоритм реализации}

Алгоритм координации на основе сигналов включает следующие этапы:
\begin{enumerate}
	\item Генерация сигналов агентами, указывающих их текущее состояние или намерения.
	\item Восприятие сигналов другими агентами.
	\item Обработка полученных сигналов и принятие решений на их основе.
	\item Адаптация поведения агентов в зависимости от поступивших сигналов.
\end{enumerate}

Блок-схема метода представлена ниже:

\begin{center}
	\text{plantuml}
\end{center}

\subsection{Примеры использования сигналов}

Примеры сигналов в системе координации включают:
\begin{itemize}
	\item Визуальные сигналы (например, изменение цвета индикаторов на роботах для передачи состояния).
	\item Акустические сигналы (например, звуковые импульсы для привлечения внимания других агентов).
	\item Радиосигналы или пакеты данных в беспроводных сетях.
\end{itemize}

\subsection{Преимущества метода}

\begin{itemize}
	\item \textbf{Простота реализации.} Использование сигналов снижает сложность алгоритмов планирования.
	\item \textbf{Отказоустойчивость.} Отсутствие зависимости от центрального координатора.
	\item \textbf{Гибкость.} Легкость адаптации системы к изменению количества агентов.
\end{itemize}

\subsection{Недостатки метода}

\begin{itemize}
	\item \textbf{Ограниченность информации.} Сигналы могут нести только ограниченное количество данных.
	\item \textbf{Шум и помехи.} Сигналы могут быть искажены или потеряны в сложных средах.
	\item \textbf{Сложность интерпретации.} Требуется разработка эффективных механизмов интерпретации сигналов.
\end{itemize}

\subsection{Применение к задачам координации агентов}

Метод координации на основе сигналов особенно полезен в ситуациях, где необходимо обеспечить быстрое и простое взаимодействие между агентами.  
Например, в задаче визуального контроля критических областей агенты могут использовать сигналы для координации движения, избегания столкновений и обновления данных о критических точках.  
Система сигналов позволяет адаптироваться к изменению задач, например, при появлении новых критических областей, которые требуют немедленного реагирования.

Метод также хорошо подходит для задач в динамичных средах, где требуется минимизировать задержки при передаче информации.
\fi