\chapter{Аналитический раздел}

\section{Постановка задачи координации агентов в многоагентной системе}
%\addcontentsline{toc}{section}{Постановка задачи координации агентов в многоагентной системе}

Координация агентов в многоагентных системах (МАС) представляет собой задачу организации взаимодействия между автономными субъектами (агентами) с целью достижения общей цели или выполнения множества задач~\cite{wooldridge2009introduction}~\cite{coordination-book}.
В данной работе задача координации рассматривается в следующих условиях:
\begin{itemize}
	\item общей целью агентов является визуальный контроль областей среды и минимизация угроз нарушения их безопасности;
	\item рассматриваемая МАС является частью игрового приложения, вследствие чего необходимо достичь внешне реалистичного поведения агентов;
	\item метод координации агентов не должен быть слишком трудоемким -- теоретическая реализация выбранного алгоритма должна быть пригодна для применения в игровых приложениях.
\end{itemize}

Рассматриваемая в рамках данной работы многоагентная система относится к следующим категориям, описанным~в~\cite{ieee-mas}:
\begin{enumerate}
	\item \textbf{Лидерство:}  
	Система является \textbf{безлидерной}, так как агенты действуют независимо, принимая решения на основе локальных данных и целей, согласованных с общей моделью.
	
	\item \textbf{Функция принятия решений:}  
	Принятие решений в данной МАС \textbf{нелинейное}, так как действия агентов зависят от сложных взаимодействий между областями, угрозами и агентами.
	
	\item \textbf{Гетерогенность:}  
	Система является \textbf{гомогенной}, так как все агенты обладают одинаковыми возможностями и характеристиками.
	
	\item \textbf{Топология:}  
	Топология системы \textbf{динамическая}, так как агенты перемещаются в пространстве и их взаимодействия изменяются в зависимости от положения и состояния среды.
	
	\item \textbf{Мобильность:}  
	Агенты в системе \textbf{мобильные}, так как перемещаются в пространстве для выполнения своих задач, таких как патрулирование и реагирование на угрозы.
\end{enumerate}

\section{Описание среды координации}
Рассмотрим формальное описание среды, в которой функционируют агенты при решении данной задачи, чтобы в дальнейшем определить наиболее применимые методы.

\textbf{1. Доступные для перемещения области}.  
Среда моделируется как множество областей $\mathcal{N} \subset \mathbb{R}^2$, доступных для перемещения агентов, описываемых навигационной картой. Навигационная карта представлена графом $G = (V, E)$~\cite{overview-of-recent}~\cite{ieee-mas}, где $V$ — множество вершин, соответствующих дискретным точкам в $\mathcal{N}$, а $E$ — множество ребер, определяющих пути перемещения между вершинами, как показано в формуле~(\ref{eq:area}):

\begin{equation}
\mathcal{N} = \bigcup_{v \in V} \mathcal{A}_v, \quad \mathcal{A}_v \subset \mathbb{R}^2,
\label{eq:area}
\end{equation}
где $\mathcal{A}_v$ — выпуклый многоугольник, ассоциированный с вершиной $v$.

\textbf{2. Препятствия, ограничивающие обзор}.  
Препятствия в среде $\mathcal{E}$ задаются множеством $\mathcal{B} = \{B_1, B_2, \dots, B_k\}$, где каждый объект $B_i$ определяется ограничивающей областью в пространстве $\mathbb{R}^2$ и определен в соответствии с формулой (\ref{eq:obstacles}):
\begin{equation}
B_i = [x_{\min}, x_{\max}] \times [y_{\min}, y_{\max}], \quad i \in \{1, 2, \dots, k\}.
\label{eq:obstacles}
\end{equation}
Присутствие объектов $\mathcal{B}$ влияет на обзор агентов, ограничивая видимость в направлении, пересекающем препятствия.

\textbf{3. Критические области}.  
Критические области, требующие визуального контроля, заданы множеством точек $\mathcal{C} = \{c_1, c_2, \dots, c_m\} \subset \mathcal{N}$.
Каждая точка $c_j$ характеризуется радиусом влияния $r_j > 0$, определяющим зону контроля, как описано в формуле (\ref{eq:critical_areas}):
\begin{equation}
\mathcal{Z}(c_j) = \{p \in \mathbb{R}^2 \mid \|p - c_j\| \leq r_j\}, \quad j \in \{1, 2, \dots, m\}.
\label{eq:critical_areas}
\end{equation}
Задача агентов заключается в том, чтобы обеспечить покрытие всех зон $\mathcal{Z}(c_j)$ при учете ограничения видимости, задаваемого препятствиями.

\textbf{4. Видимость в среде}.  
Модель видимости агента $a_i$ определяется его текущим положением $p_i \in \mathcal{N}$ и углом обзора $\phi_i$.
Область видимости агента формируется как сектор окружности, определяемый формулой (\ref{eq:visibility}):
\begin{equation}
\mathcal{V}(p_i, \phi_i, r_{\text{max}}) = \{p \in \mathbb{R}^2 \mid \|p - p_i\| \leq r_{\text{max}}, \, \theta(\dot{p}_i(t), p) \leq \phi_i\},
\label{eq:visibility}
\end{equation}
где $r_{\text{max}}$ — максимальная дальность обзора, $\theta(\dot{p}_i(t), p)$ — угол между направлением агента и вектором к точке $p$.

Таким образом, среда представляет собой совокупность доступных областей $\mathcal{N}$, препятствий $\mathcal{B}$ и критических точек $\mathcal{C}$, которые агенты обязаны контролировать с учетом ограничений видимости.
Основная задача координации заключается в определении таких траекторий и позиций агентов, которые минимизируют неохваченные зоны $\mathcal{Z}(c_j)$.

\textbf{5. Агенты}.  
Агенты в системе заданы множеством $\mathcal{A} = \{a_1, a_2, \dots, a_n\}$.
Каждый агент $a_i$ характеризуется следующими параметрами:  
\begin{itemize}[leftmargin=1.6\parindent]
	\item текущей позицией $p_i(t) \in \mathcal{N}$ в момент времени $t$;  
	\item направлением движения $\theta_i(t) \in [0, 2\pi)$;  
	\item скоростью перемещения $v_i(t) \in [0, v_{\text{max}}]$.
\end{itemize}

Траектория движения агента задается уравнением (\ref{eq:agent_movement}):  
\begin{equation}
\dot{p}_i(t) = v_i(t) \cdot \begin{bmatrix}
	\cos(\theta_i(t)) \\
	\sin(\theta_i(t))
\end{bmatrix}.
\label{eq:agent_movement}
\end{equation}

\textbf{6. Угрозы}.  
Угрозы представлены множеством $\mathcal{T}_{\text{threat}} = \{\tau_1, \tau_2, \dots, \tau_k\}$, где каждая угроза $\tau_j$ является динамическим объектом, имеющим параметры:  
\begin{itemize}[leftmargin=1.6\parindent]
	\item позицию $q_j(t) \in \mathcal{N}$ в момент времени $t$;  
	\item направление движения $\phi_j(t) \in [0, 2\pi)$;  
	\item скорость $v_j(t) \in [0, v_{\text{threat}}]$.;
	\item радиус влияния угрозы $r_{\tau}$.
\end{itemize}

Движение угроз также описывается уравнением (\ref{eq:agent_movement}).

Достижение угрозой одной из критических точек $c \in \mathcal{C}$ считается нарушением безопасности и должно быть предотвращено агентами.

\textbf{7. Динамическое патрулирование}.  
Для обеспечения контроля над всей областью $\mathcal{N}$ агенты реализуют стратегию динамического патрулирования.
При этом вводится функция \textit{опасности} $U(p, t)$, описывающая степень опасности в точке $p \in \mathcal{N}$ в момент времени $t$. 

Общая динамика изменения $U(p, t)$ описывается уравнением (\ref{eq:danger_function_patrol}):
\begin{equation}
	\frac{\partial U(p, t)}{\partial t} = \alpha - \beta \sum_{i=1}^n \mathbf{1}\{p \in \mathcal{V}(p_i(t), \phi_i(t), r_{\text{max}})\},
	\label{eq:danger_function_patrol}
\end{equation}
где:
$\alpha > 0$ — скорость естественного роста опасности в непосещаемых зонах;
$\beta > 0$ — скорость снижения опасности за счет патрулирования агентами.

В области с радиусом $r_\tau$ вокруг угрозы значение функции опасности увеличивается согласно формуле (\ref{eq:danger_function_threat}):
\begin{equation}
	U(p, t) = U(p, t) + \gamma \cdot \mathbf{1}\{\|p - q_\tau(t)\| \leq r_\tau\},
	\label{eq:danger_function_threat}
\end{equation}
где:
$\gamma > 0$ — интенсивность роста опасности, связанная с угрозой $\tau$;
$r_\tau$ — радиус влияния угрозы $\tau$.

Таким образом, значение функции опасности увеличивается в моменте, когда угроза находится рядом, и прекращает расти, как только угроза удаляется.

\textbf{8. Задача агентов}.  
Задача агентов заключается в следующем:  
\begin{itemize}
	\item минимизировать функцию общей опасности (\ref{eq:total_threat}):
	\begin{equation}
	U_{\text{total}}(t) = \int_{\mathcal{N}} U(p, t) \, dp;
	\label{eq:total_threat}
	\end{equation}
	\item обнаруживать угрозы $\tau_j \in \mathcal{T}_{\text{threat}}$ и предотвращать их достижение критических точек $\mathcal{C}$.
\end{itemize}

Если угроза $\tau_j$ обнаружена агентом $a_i$, то остальные агенты $a_{k \neq i}$ координируют свои действия для перехвата угрозы, чтобы нейтрализовать ее и предотвратить достижение критической области.

\chapter{Обзор методов координации агентов в многоагентных системах}

\iffalse
В разделе приведена разработанная классификация методов координации агентов, а также описаны известные методы применительно к рассматриваемой задаче~\cite{overview-of-recent}.
\fi

\section{Классификация методов координации агентов в многоагентных системах}

В данной работе классификация методов проводится с учетом особенностей задачи визуального покрытия критических областей, а также требований к моделированию в игровых приложениях~\cite{mas-game-engine}. Для описания методов предлагается следующая классификация:

\subsubsection*{1. Тип взаимодействия между агентами}

Методы координации могут быть разделены на централизованные и децентрализованные~\cite{ieee-mas}:
\begin{itemize}
	\item \textbf{Централизованные методы}: предполагают наличие центрального узла, который координирует действия всех агентов.
	Такие методы обеспечивают глобальную оптимальность решений, но требуют глобальной видимости среды и ее полный анализ;
	\item \textbf{Децентрализованные методы}: агенты принимают решения только на основе полученной ими информации и действуют независимо от других.
	Эти методы более устойчивы к сбоям и масштабируемы, но могут приводить к субоптимальным решениям.
\end{itemize}

\subsubsection*{2. Область восприятия агента}

Методы различаются по тому, какую часть среды может учитывать агент при принятии решений~\cite{ieee-mas}:
\begin{itemize}
	\item \textbf{Методы с глобальным восприятием}: агенты имеют доступ к информации обо всей среде, включая местоположение всех критических точек, других агентов и угроз.
	Это требует высокой вычислительной мощности и связности сети.  
	\item \textbf{Методы с локальным восприятием}: решения принимаются на основе информации из ограниченной области вокруг агента.
	Это снижает требования к вычислительным ресурсам, но может увеличить риск неполного охвата критических точек.
\end{itemize}

\subsubsection*{3. Способ распределения задач между агентами}

Эффективность координации зависит от способа распределения задач~\cite{role-based}:
\begin{itemize}
	\item \textbf{Жестко распределенные задачи}: каждому агенту заранее назначается определенная область или роль, что упрощает координацию, но снижает адаптивность к изменяющимся условиям.
	\item \textbf{Динамическое распределение задач}: задачи перераспределяются в процессе выполнения на основе текущей информации о среде.
	Этот подход обеспечивает большую гибкость, но требует дополнительных вычислений.
\end{itemize}

\subsubsection*{4. Сложность алгоритмов реализации}

Хотя сложность конкретных алгоритмов реализации методов координации агентов может различаться, для сравнения методов полезной является приблизительная оценка трудоёмкости вычисления методов в зависимости от основных параметров модели:
\begin{itemize}
	\item $a$ — количество агентов в системе. Это определяет степень взаимодействия между агентами и влияет на необходимость синхронизации и перерасчёта их состояний.
	\item $v$ — количество вершин графа среды. Вершины представляют возможные положения агентов и критических точек.
	\item $e$ — количество рёбер графа среды, связанное с числом вершин соотношением $e \sim c \cdot v$, где $c$ — среднее число связей для каждой вершины.
	\item $b$ — количество препятствий, влияющих на построение областей видимости и маршрутов.
	\item $t$ — количество угроз -- вражеских объектов, которые нужно обнаружить и нейтрализовать.
	\item $c$ — количество критических областей, требующих визуального контроля.
\end{itemize}

Сводя эти параметры, можно выделить минимально необходимые переменные для оценки сложности: 
\begin{itemize}
	\item $a$ — количество агентов;
	\item $v$ — количество вершин графа среды;
	\item $c$ - количество критических областей;
	\item $b$ — количество препятствий;
	\item $t$ — количество угроз.
\end{itemize}

Оценка трудоемкости производится в нотации <<О>> большое для сравнения асимптотического поведения функций.

\subsubsection*{5. Гибкость метода к изменяющимся условиям}

Из-за того, что среда является высоко динамичной за счет наличия движущихся агентов и изменяющегося уровня опасности, методы могут быть классифицированы по гибкости к изменяющимся условиям следующим образом:
\begin{itemize}
	\item \textbf{Методы с низкой гибкостью}: эффективно работают в статических средах, но требуют значительного времени для перерасчета при изменении условий.  
	\item \textbf{Методы с высокой гибкостью}: автоматически корректируют действия агентов в ответ на изменения в среде, что делает их подходящими для задач в динамических игровых приложениях.
\end{itemize}

\subsubsection*{6. Визуальная правдоподобность движений агентов}

Для игровых приложений важно, чтобы движения агентов выглядели естественно с точки зрения игрока.
Методы могут быть классифицированы по уровню визуальной правдоподобности:
\begin{itemize}
	\item \textbf{Прямолинейные методы}: движения агентов строго следуют оптимальной траектории.
	Это может быть эффективно с точки зрения минимизации затрат, но выглядит механистично и неестественно.  
	\item \textbf{Методы с естественным поведением}: включают элементы непредсказуемости или плавности в траекториях агентов, что улучшает восприятие их действий игроком.
\end{itemize}

\section{Методы координации агентов в многоагентных системах}

\subsection{Метод потенциальных полей}
Метод потенциальных полей основывается на вычислении градиента искусственного потенциала, который направляет движение агентов~\cite{rts-potential-fields}.
Для применения к нашей задаче метод должен учитывать следующие элементы: препятствия, критические области, угрозы, а также общую функцию опасности.

\subsubsection*{Общее описание метода потенциалов}
Потенциал для агента $a_i$ задается как функция (\ref{eq:potential}):
\begin{equation}
	\label{eq:potential}
	V(p_i) = V_{\text{аттрактор}}(p_i) + V_{\text{репеллент}}(p_i),
\end{equation}
где $p_i$ — позиция агента.  
Компоненты потенциала определяются следующим образом:  
\begin{itemize}
	\item $V_{\text{акттрактор}}(p_i)$ — компонент, притягивающий агента к целевым областям (например, критическим точкам).
	\item $V_{\text{репеллент}}(p_i)$ — компонент, отталкивающий агента от препятствий, других агентов и областей с высоким уровнем опасности.
\end{itemize}

Градиент потенциала (\ref{eq:potential_gradient}) определяет направление движения агента:
\begin{equation}
	\label{eq:potential_gradient}
	\dot{p}_i = -\nabla V(p_i),
\end{equation}
где $\dot{p}_i$ — скорость агента.

\subsubsection*{Адаптация метода к задаче}
Для нашей задачи потенциал должен учитывать:
\begin{itemize}[leftmargin=1.6\parindent]
	\item Привлечение агентов к критическим точкам и областям с высокой функцией опасности.
	\item Отталкивание агентов от препятствий и других агентов.
	\item Отталкивание агентов от областей с высокой плотностью угроз.
\end{itemize}

Потенциал определяется по формуле (\ref{eq:potential_final}):
\begin{equation}
	\label{eq:potential_final}
	V(p_i) = \sum_{k=1}^{c} w_k \cdot V_{\text{кр}}(p_i, q_k) + \sum_{j=1}^{b} w_j \cdot V_{\text{преп}}(p_i, o_j) + \sum_{\tau=1}^{t} w_\tau \cdot V_{\text{угр}}(p_i, \tau),
\end{equation}
где:  
\begin{itemize}[leftmargin=1.6\parindent]
	\item $V_{\text{кр}}(p_i, q_k)$ — аттрактор (\ref{eq:potential_attr}), притягивающий $a_i$ к критической точке $q_k$.
	\item $V_{\text{преп}}(p_i, o_j)$ — репеллент (\ref{eq:potential_blocks}), отталкивающий $a_i$ от препятствия $o_j$.
	\item $V_{\text{угр}}(p_i, \tau)$ — репеллент (\ref{eq:potential_threats}), отталкивающий $a_i$ от угрозы $\tau$.
	\item $w_k, w_j, w_\tau$ — весовые коэффициенты.
\end{itemize}

Каждая компонента определяется как:
\begin{align}
	V_{\text{кр}}(p_i, q_k) &= -\frac{1}{\|p_i - q_k\| + \epsilon}, \label{eq:potential_attr} \\
	V_{\text{преп}}(p_i, o_j) &= \frac{1}{\|p_i - o_j\|^2 + \epsilon}, \label{eq:potential_blocks} \\
	V_{\text{угр}}(p_i, \tau) &= \frac{1}{\|p_i - \tau\|^2 + \epsilon}, \label{eq:potential_threat}
\end{align}
где $\epsilon > 0$ предотвращает деление на ноль.

\subsubsection*{Алгоритмическая сложность метода}
Сложность метода определяется трудоемкостью вычислений потенциала для каждого агента. 

Требуется учесть каждую критическую точку $q_k$, препятствие $o_j$ и угрозу $\tau$, а также влияние функции опасности.

Итоговая сложность для одного агента характеризуется формулой~(\ref{eq:potential_complexity_one}):
\begin{equation}
	\label{eq:potential_complexity_one}
	O_{\text{агент}} = O(c + b + t).
\end{equation}

Суммарная сложность для всех агентов характеризуется формулой~(\ref{eq:potential_complexity}):
\begin{equation}
	\label{eq:potential_complexity}
	O_{\text{общая}} = O(a \cdot (c + b + t)).
\end{equation}

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{децентрализованным}, так как каждый агент принимает решения на основе локальных вычислений потенциала.  

\textbf{Область восприятия:}  
Метод использует \textbf{локальное восприятие}, ограниченное областью действия потенциала~\cite{bvp-planning}.

\textbf{Распределение задач:}  
Задачи распределяются \textbf{динамически} в процессе вычисления градиента.  

\textbf{Сложность:}  
Итоговая сложность $O(a \cdot (c + b + t))$ является линейной относительно числа агентов $a$ и элементов среды.
Это позволяет применять метод в режиме реального времени.  

\textbf{Гибкость:}  
Метод обладает \textbf{высокой адаптивностью}, так как параметры потенциалов можно изменять в зависимости от текущих условий.

\textbf{Правдоподобность:}  
Метод обеспечивает \textbf{высокий уровень правдоподобности} при тщательной настройке параметров, что показано в \cite{rts-potential-fields}.

\subsection{Метод ролей}

Метод ролей основывается на назначении фиксированных функций или ролей агентам, которые определяют их поведение и задачи в системе~\cite{role-based}.

Для применения к нашей задаче данный метод должен учитывать распределение агентов по функциям патрулирования, защиты критических областей и нейтрализации угроз.

\subsubsection*{Общее описание метода ролей}
В методе ролей каждому агенту $a_i$ назначается роль $r_i$ из множества допустимых ролей $R$, определяемого формулой (\ref{eq:roles}):
\begin{equation}
	\label{eq:roles}
	r_i \in R = \{\text{патрулирование}, \text{защита}, \text{перехват}\}.
\end{equation}

Каждая роль имеет свои задачи:
\begin{itemize}
	\item \textbf{Патрулирование:} агент перемещается по маршруту, покрывающему определенную область.
	\item \textbf{Защита:} агент остается вблизи критической точки и контролирует угрозы в ее окружении.
	\item \textbf{Перехват:} агент направляется к обнаруженной угрозе для ее нейтрализации.
\end{itemize}

Назначение ролей может быть статическим (фиксированное распределение) или динамическим (меняется в зависимости от ситуации).
В рассматриваемой задаче применимо \textbf{динамическое распределение}, где роли пересматриваются в реальном времени на основе состояния среды.

\subsubsection*{Адаптация метода к задаче}
Адаптация метода ролей требует учета следующих факторов:
\begin{itemize}
	\item Выбор ролей агентов в зависимости от текущего уровня опасности $\mathcal{U}(x, y, t)$, положения критических точек и угроз.
	\item Оптимизация распределения ролей для минимизации времени реакции на угрозы и покрытия областей.
\end{itemize}

Процесс распределения ролей описывается формулой (\ref{eq:role_assignment}):
\begin{equation}
	r_i = \arg \min_{r \in R} C(r, p_i, \mathcal{U}, T),
	\label{eq:role_assignment}
\end{equation}
где $C(r, p_i, \mathcal{U}, T)$ — функция стоимости назначения роли $r$ агенту $a_i$, зависящая от его положения $p_i$, функции опасности $\mathcal{U}$ и текущего набора угроз $T$.

Функция стоимости для ролей определяется формулами~(\ref{eq:role_patrol})-(\ref{eq:role_catch}):
\begin{align}
	C_{\text{патрулирование}} &= \sum_{(x, y) \in A_i} \mathcal{U}(x, y, t), \label{eq:role_patrol} \\
	C_{\text{защита}} &= \sum_{q_k \in Q} \frac{1}{\|p_i - q_k\| + \epsilon}, \label{eq:role_defend} \\
	C_{\text{перехват}} &= \min_{\tau \in T} \frac{\|p_i - \tau\|}{v_i}, \label{eq:role_catch}
\end{align}
где:
\begin{itemize}[leftmargin=1.6\parindent]
	\item $A_i$ — область, закрепленная за агентом $a_i$ для патрулирования.
	\item $Q$ — множество критических точек.
	\item $v_i$ — скорость агента $a_i$.
\end{itemize}

\subsubsection*{Алгоритмическая сложность метода}
Рассмотрим сложность распределения ролей.  
\begin{itemize}
	\item Для патрулирования необходимо вычислить сумму значений функции опасности по закрепленной области $A_i$, что требует $O(v)$ операций (по числу вершин графа).
	\item Для защиты требуется рассчитать расстояние до всех критических точек, что требует $O(c)$ операций.
	\item Для перехвата необходимо вычислить расстояние до всех угроз, что требует $O(t)$ операций.
\end{itemize}

Итоговая сложность распределения ролей для одного агента характеризуется формулой (\ref{eq:role_complexity_one}):
\begin{equation}
	\label{eq:role_complexity_one}
	O_{\text{агент}} = O(v + c + t).
\end{equation}

Суммарная сложность для всех агентов оцененивается формулой~(\ref{eq:role_complexity}):
\begin{equation}
	\label{eq:role_complexity}
	O_{\text{общая}} = O(a \cdot (v + c + t)).
\end{equation}

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{централизованным}, так как роли назначаются всем агентам на основе глобальных данных о среде.

\textbf{Область восприятия:}  
Метод использует \textbf{глобальное восприятие}, так как распределение ролей требует информации о всей системе.  

\textbf{Распределение задач:}  
Распределение задач является \textbf{динамическим}, так как роли пересматриваются на основе текущего состояния среды.  

\textbf{Сложность:}  
Сложность $O(a \cdot (v + c + t))$ является линейной относительно числа агентов $a$, что позволяет использовать метод в режиме реального времени, если количество вершин $v$ остается не слишком большим.  

\textbf{Гибкость:}  
Метод обладает \textbf{высокой гибкостью}, так как роли могут быть адаптированы к изменениям в среде.  

\textbf{Правдоподобность:}  
Метод обеспечивает \textbf{низкий уровень правдоподобности}, так как агенты обладают ограниченным количеством паттернов поведения, которое задается числом ролей.

\subsection{Метод роя частиц}

Метод роя -- это метод роевого интеллекта, используемый для оптимизации сложных нелинейных целевых функций, основанный на итеративном улучшении решения-кандидата с учетом заданного показателя качества~\cite{particle-swarm}.

\subsubsection*{Общее описание метода роя}
Поведение каждого агента $a_i$ определяется рядом локальных правил:  

1. \textbf{Притяжение:} движение к центру масс соседних агентов описывается формулой (\ref{eq:swarm_attract}):  
\begin{equation}
	\label{eq:swarm_attract}
	f_{\text{притяжение}}(p_i) = k_{\text{пр}} \cdot \left( \frac{1}{|N_i|} \sum_{a_j \in N_i} p_j - p_i \right),
\end{equation}
где $N_i$ — множество агентов в радиусе восприятия $r_{\text{воспр}}$, $k_{\text{пр}}$ — коэффициент притяжения.

2. \textbf{Избегание:} отталкивание от слишком близко расположенных агентов описывается формулой (\ref{eq:swarm_discard}):  
\begin{equation}
	\label{eq:swarm_discard}
	f_{\text{избегание}}(p_i) = \sum_{a_j \in N_i^{\text{близ}}} k_{\text{изб}} \cdot \frac{p_i - p_j}{\|p_i - p_j\|^3},
\end{equation}
где $N_i^{\text{близ}}$ — множество агентов в пределах малого радиуса $r_{\text{мин}}$, $k_{\text{изб}}$ — коэффициент избегания.

3. \textbf{Выравнивание:} согласование направления движения описывается формулой (\ref{eq:swarm_smooth}):  
\begin{equation}
	\label{eq:swarm_smooth}
	f_{\text{выравнивание}}(p_i) = k_{\text{выр}} \cdot \left( \frac{1}{|N_i|} \sum_{a_j \in N_i} \dot{p}_j - \dot{p}_i \right),
\end{equation}
где $k_{\text{выр}}$ — коэффициент выравнивания.

Суммарное движение агента может быть описано формулой (\ref{eq:swarm_total}):
\begin{equation}
	\label{eq:swarm_total}
	\dot{p}_i = f_{\text{притяжение}}(p_i) + f_{\text{избегание}}(p_i) + f_{\text{выравнивание}}(p_i).
\end{equation}

\subsubsection*{Адаптация метода к задаче}
Для нашей задачи метод роя модифицируется следующим образом:  
1. \textbf{Учет функции опасности $\mathcal{U}(x, y, t)$}: агенты перемещаются в области, где $\mathcal{U}$ имеет высокие значения.
Это обеспечивается добавлением аттрактора (\ref{eq:swarm_attractor}):
\begin{equation}
	\label{eq:swarm_attractor}
	f_{\text{опасность}}(p_i) = -k_{\text{оп}} \cdot \nabla \mathcal{U}(p_i, t),
\end{equation}
где $k_{\text{оп}}$ — коэффициент чувствительности к опасности.

2. \textbf{Нейтрализация угроз}: агенты в области видимости угроз $\tau$ перенаправляются к ним.
Дополнительное движение определяется как (\ref{eq:swarm_threat}):
\begin{equation}
	\label{eq:swarm_threat}
	f_{\text{угроза}}(p_i) = -k_{\text{угр}} \cdot \sum_{\tau \in T_{\text{вид}}} \frac{p_i - \tau}{\|p_i - \tau\|^3},
\end{equation}
где $T_{\text{вид}}$ — множество угроз в радиусе видимости агента.

3. \textbf{Балансирование покрытия и плотности}: для предотвращения скопления агентов используется штраф за высокую плотность (\ref{eq:swarm_balance}):
\begin{equation}
	\label{eq:swarm_balance}
	f_{\text{разрежение}}(p_i) = k_{\text{разр}} \cdot \left( \frac{1}{|N_i|} - \rho_{\text{целевая}} \right),
\end{equation}
где $\rho_{\text{целевая}}$ — целевая плотность агентов.

Итоговое движение агента описывается формулой (\ref{eq:swarm_final}):
\begin{equation}
	\label{eq:swarm_final}
	\dot{p}_i^{total} =\dot{p}_i + f_{\text{опасность}}(p_i) + f_{\text{угроза}}(p_i) + f_{\text{разрежение}}(p_i).
\end{equation}

\subsubsection*{Алгоритмическая сложность метода}
Сложность метода роя определяется числом соседей каждого агента.
Обозначим среднее число соседей как $|N_i|$.

\begin{itemize}
	\item На вычисление взаимодействий для одного агента требуется $O(|N_i|)$.
	\item Учет функции опасности требует $O(v)$ операций для каждого агента, так как $\mathcal{U}$ задается на графе.
	\item Для обнаружения угроз в радиусе видимости необходимо $O(t)$.
\end{itemize}

Итоговая сложность для одного агента характеризуется формулой~(\ref{eq:swarm_complexity_one}):
\begin{equation}
	\label{eq:swarm_complexity_one}
	O_{\text{агент}} = O(|N_i| + v + t).
\end{equation}

Общая сложность характеризуется формулой~(\ref{eq:swarm_complexity}):
\begin{equation}
	\label{eq:swarm_complexity}
	O_{\text{общая}} = O(a \cdot (|N_i| + v + t)).
\end{equation}

При фиксированном радиусе восприятия $r_{\text{воспр}}$, трудоемкость остается практически постоянной, что делает метод подходящим для реального времени.

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{децентрализованным}, так как агенты принимают решения на основе локальной информации~\cite{swarm-based-coord}.  

\textbf{Область восприятия:}  
Метод использует \textbf{локальное восприятие}, ограниченное радиусом $r_{\text{воспр}}$.  

\textbf{Распределение задач:}  
Задачи распределяются \textbf{динамически} на основе взаимодействия с функцией опасности и угрозами.  

\textbf{Сложность:}  
Сложность $O(a \cdot (|N_i| + v + t))$ линейна относительно числа агентов $a$ и остается практически константной относительно $|N_i|$, что подходит для игр в реальном времени.  

\textbf{Гибкость:}  
Метод обладает \textbf{низкой гибкостью}, так как поведения агентов в основном продиктовано необходимостью группироваться и повторять действия соседних агентов, реагирующих на локальные изменения среды. 

\textbf{Правдоподобность:}
Метод обеспечивает \textbf{высокий уровень правдоподобности}, так как движение агентов обладает свойствами естественного поведения роя~\cite{swarm-based-coord}.

\subsection{Метод планирования на основе теории игр}

Метод планирования на основе теории игр предполагает, что агенты взаимодействуют, решая задачи оптимального поведения в многоагентной среде через формирование и решение математической модели игры~\cite{gurevich2005multiagent}.

\subsubsection*{Общее описание метода}
Модель задачи представляется как стратегическая игра $(A, U)$, где:  
\begin{itemize}
	\item $A = \{A_1, A_2, \dots, A_a\}$ — множество агентов;
	\item $S_i$ — множество стратегий $i$-го агента;
	\item $U_i: S_1 \times S_2 \times \dots \times S_a \to \mathbb{R}$ — функция выигрыша $i$-го агента, зависящая от стратегий всех агентов.
\end{itemize}

В ходе игры каждый агент выбирает стратегию $s_i \in S_i$, стремясь максимизировать свою функцию выигрыша $U_i$~\cite{parsons2002gametheory}.
Решение задачи игры определяется через нахождение равновесий, например, равновесия Нэша, которые удовлетворяют условию (\ref{eq:nash_condition}):  
\begin{equation}
	\label{eq:nash_condition}
	U_i(s_{-i}^*, s_i^*) \geq U_i(s_{-i}^*, s_i) \quad \forall s_i \in S_i,
\end{equation}
где $s_{-i}^*$ — стратегии всех агентов, кроме $i$, в равновесии.

\subsubsection*{Адаптация метода к задаче}
Для нашей задачи метод планирования на основе теории игр модифицируется следующим образом:  

1. \textbf{Множество стратегий}:
Каждый агент выбирает маршрут и целевую область покрытия.
Множество стратегий $S_i$ для агента $i$ включает все возможные пути вдоль графа среды, ведущие к областям покрытия.  

2. \textbf{Функция выигрыша}:  
Функция выигрыша $U_i$ определена как~(\ref{eq:win_function}):
\begin{equation}
	\label{eq:win_function}
	U_i(s_i, s_{-i}) = - \alpha \mathcal{U}(p_i) - \beta \sum_{c_k \in C} \mathcal{U}(c_k) + \gamma \sum_{\tau \in T} d(p_i, \tau),
\end{equation}
где:
\begin{itemize}
	\item $\mathcal{U}(p_i)$ — значение функции опасности в целевом положении $p_i$ агента;
	\item $\mathcal{U}(c_k)$ — значение функции опасности в критической области $c_k$;
	\item $d(p_i, \tau)$ — расстояние до угрозы $\tau$;
	\item $\alpha, \beta, \gamma$ — веса, задающие приоритеты поведения.
\end{itemize}

3. \textbf{Решение игры}:  
Игра решается в реальном времени через итеративное приближение равновесия Нэша~\cite{parsons2002gametheory}.
Для этого каждый агент оптимизирует свою стратегию $s_i$, исходя из стратегий остальных агентов $s_{-i}$ согласно (\ref{eq:strategy_optimization}).
\begin{equation}
	\label{eq:strategy_optimization}
	s_i^* = \arg\max_{s_i \in S_i} U_i(s_i, s_{-i}^*).
\end{equation}

4. \textbf{Нейтрализация угроз}:  
Если угроза $\tau$ находится в области видимости агента $i$, стратегия агента автоматически переходит к ее преследованию и нейтрализации выбирая стратегию по формуле (\ref{eq:strategy_threat}):
\begin{equation}
	\label{eq:strategy_threat}
	s_i = \arg\min_{s_i \in S_i} d(p_i, \tau).
\end{equation}

\subsubsection*{Алгоритмическая сложность метода}
Сложность метода определяется числом агентов, стратегий и итераций поиска равновесия:
\begin{itemize}
	\item Для каждого агента построение множества стратегий $S_i$ требует $O(v^2)$, при применении алгоритма Дейкстры для построения кратчайших путей.
	\item Оценка функции выигрыша $U_i$ для всех стратегий $S_i$ требует $O(|S_i|)$.
	\item Поиск равновесия итеративным методом (например, методом наивной оптимизации) требует $O(k \cdot a \cdot |S_i|)$, где $k$ — число итераций до сходимости.
\end{itemize}

При этом можно принять, что $|S_i| \leq v^2$ и $k=const$, тогда итоговая сложность характеризуется формулой (\ref{eq:game_complexity}):
\begin{equation}
	\label{eq:game_complexity}
	O_{\text{общая}} = O(a \cdot v^2).
\end{equation}

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод относится к \textbf{централизованному типу}, так как агенты используют информацию о стратегиях, выбранных другими агентами~\cite{gurevich2005multiagent}.

\textbf{Область восприятия:}  
Метод использует \textbf{локальное восприятие}, так как стратегии формируются на основе окружения агента.

\textbf{Распределение задач:}  
Задачи распределяются \textbf{статически}, поскольку стратегия выбирается на основе рассчитаного оптимума~\cite{gametheory-applcation}.

\textbf{Сложность:}  
Метод имеет сложность $O(a \cdot v^2)$, что делает его трудоемким для сред с большим размером локации.

\textbf{Гибкость:}  
Метод обладает \textbf{высокой} гибкостью, поскольку стратегия агента адаптируется на основе его окружения.

\textbf{Правдоподобность:}  
Правдоподобность метода \textbf{высокая}, так как поведение агентов, основанное на игровых стратегиях, соответствует ожиданиям от разумной координации~\cite{gametheory-applcation}.

\subsection{Метод на основе обучения с подкреплением}

\subsubsection*{Общее описание метода}
Метод Q-обучения относится к виду методов обучения с подкреплением.
Данный метод позволяет моделировать поведение агентов как процесс последовательного принятия решений в среде.
Среда представляется в виде марковского процесса принятия решений~\cite{markov-decision-process}, который задается пятеркой $(S, A, P, R, \gamma)$:
\begin{itemize}[leftmargin=1.6\parindent]
	\item $S$ — множество состояний среды,
	\item $A$ — множество возможных действий агента,
	\item $P(s' | s, a)$ — функция переходов между состояниями при выполнении действия $a$,
	\item $R(s, a)$ — функция вознаграждения за выполнение действия $a$ в состоянии $s$,
	\item $\gamma \in [0, 1]$ — коэффициент дисконтирования будущих вознаграждений.
\end{itemize}

Цель агента заключается в максимизации ожидаемой суммарной дисконтированной награды (\ref{eq:qlearn_reward}):
\begin{equation}
	\label{eq:qlearn_reward}
	G_t = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k}) \right].
\end{equation}

Агент учится стратегии $\pi(a|s)$, которая задает вероятность выбора действия $a$ в состоянии $s$.
Обучение стратегии происходит на основе значений функции полезности (\ref{eq:qlearn_qfunc}):
\begin{equation}
	\label{eq:qlearn_qfunc}
	Q^\pi(s, a) = \mathbb{E} \left[ \sum_{k=0}^{\infty} \gamma^k R(s_{t+k}, a_{t+k}) \bigg| s_t = s, a_t = a \right].
\end{equation}

\subsubsection*{Адаптация метода к задаче}
Для задачи визуального покрытия критических областей вводится специфическая структура состояния, действий, и функции награды:
\begin{itemize}
	\item \textbf{Состояния ($S$):} Состояние агента включает:
	\begin{itemize}
		\item Текущую позицию $p_i$ и скорость $\dot{p}_i$ агента,
		\item Значения функции опасности $\mathcal{U}(x, y, t)$ в окрестности агента,
		\item Расположение ближайших критических точек и угроз.
	\end{itemize}
	\item \textbf{Действия ($A$):} Агенты могут выбирать движение в одном из $k$ направлений, задаваемых дискретизацией пространства, или оставаться на месте.
	\item \textbf{Функция вознаграждения ($R$):} Вознаграждение определяется функцией (\ref{eq:qlearn_rewardfunc}):
	\begin{equation}
		\label{eq:qlearn_rewardfunc}
		R(s, a) = -\alpha \cdot \mathcal{U}(p_i, t) + \beta \cdot I_{\text{угроза\_нейтрализована}} - \gamma \cdot \mathcal{L}(p_i),
	\end{equation}
	где $\mathcal{L}(p_i)$ — штраф за выход за границы области, $\alpha, \beta, \gamma$ — коэффициенты весов, $I_{\text{угроза\_нейтрализована}}$ — индикатор нейтрализации угрозы.
\end{itemize}

Для обучения стратегии используется симуляция среды: агенты взаимодействуют с функцией опасности $\mathcal{U}(x, y, t)$, перемещаются между вершинами графа и реагируют на появление угроз $\tau$.
Модель среды обновляется согласно описанной динамике.

\subsubsection*{Алгоритмическая сложность метода}
Обучение с подкреплением включает два основных этапа:

1. \textbf{Симуляция среды:} При фиксированном числе агентов $a$ симуляция одного шага занимает $O(a \cdot v)$ операций, так как необходимо обновить функцию $\mathcal{U}$ и обработать поведение каждого агента.

2. \textbf{Обновление стратегии:} Для алгоритма Q-обучения требуется обновление таблицы $Q(s, a)$, что занимает $O(|S| \cdot |A|)$ операций.

При использовании Deep Q-Learning сложность определяется числом параметров нейронной сети $n_{\text{параметры}}$.

Значения $|S|$, $|A|$, $n_{\text{параметры}}$ фиксированы, так что итоговая сложность обучения характеризуется формулой (\ref{eq:qlearn_complexity}):
\begin{equation}
	\label{eq:qlearn_complexity}
	O_{\text{обучение}} = O(N \cdot (a \cdot v + |S| \cdot |A|)),
\end{equation}
где $N$ — число шагов симуляции.

Применение обученной стратегии в реальном времени требует $O(a)$ операций на каждом шаге.

\subsubsection*{Классификация метода}
\textbf{Тип взаимодействия:}  
Метод является \textbf{децентрализованным}, агенты обучаются индивидуально и учитывают окружение через локальное состояние. 

\textbf{Область восприятия:}  
Метод использует \textbf{локальное восприятие}, так как состояние включает информацию об окружении агента и его параметрах.

\textbf{Распределение задач:}  
Распределение задач \textbf{статическое} во время выполнения, так как стратегия фиксируется после этапа обучения~\cite{markov-decision-process}.  

\textbf{Сложность:}  
Обучение требует больших вычислительных ресурсов ($O_{\text{обучение}}$), однако применение стратегии после обучения не требует трудоемких вычислений.

\textbf{Гибкость:}  
Метод обладает \textbf{высокой гибкостью}, так как стратегия может адаптироваться к сложным динамическим сценариям.  

\textbf{Правдоподобность:}  
Метод обеспечивает \textbf{высокую правдоподобность}, так как обученные стратегии могут воспроизводить реалистичное поведение при корректной настройке параметров~\cite{hysteretic-q-learning}.

\chapter{Сравнение методов координации агентов в многоагентных системах}

На основе классификации методов, описанных выше, проведем сравнительный анализ по критериям, сформулированным ранее. В таблицах \ref{tab:comparison1}-\ref{tab:comparison2} приведены ключевые характеристики методов координации.

\begin{table}[H]
	\centering
	\caption{Сравнение методов координации агентов в МАС (часть 1/2)}
	\label{tab:comparison1}
	\begin{adjustbox}{width=\textwidth}
		\begin{tabular}{|c|c|c|c|}
		\hline
		\textbf{Метод} & \makecell{\textbf{Тип}\\\textbf{взаимодействия}} & \makecell{\textbf{Область}\\\textbf{восприятия}} & \makecell{\textbf{Распределение}\\\textbf{задач}} \\ \hline
		Потенциальных полей    & Децентрализованный         & Локальная                  & Динамическое                 \\ \hline
		Ролей          & Централизованный            & Глобальная                 & Динамическое                 \\ \hline
		Роя частиц     & Децентрализованный         & Локальная                  & Динамическое                 \\ \hline
		Теоретико-игровой     & Централизованный           & Локальная                 & Статическое                  \\ \hline
		Обучения с подкреплением & Децентрализованный         & Локальная                 & Статическое                  \\ \hline
		\end{tabular}
	\end{adjustbox}
\end{table}

\begin{table}[H]
	\centering
	\caption{Сравнение методов координации агентов в МАС (часть 2/2)}
	\label{tab:comparison2}
	\begin{adjustbox}{width=\textwidth}
		\begin{tabular}{|c|c|c|c|}
			\hline
			\textbf{Метод} & \makecell{\textbf{Сложность}} & \makecell{\textbf{Гибкость}} & \makecell{\textbf{Правдоподобность}} \\ \hline
			Потенциальных полей    & $O(a \cdot (c + b + t))$   & Высокая                    & Высокая                      \\ \hline
			Ролей          & $O(a \cdot (v + c + t))$   & Высокая                    & Низкая                      \\ \hline
			Роя частиц     & $O(a \cdot (|N_i| + v + t))$ & Низкая                     & Высокая                      \\ \hline
			Теоретико-игровой & $O(a \cdot v^2)$         & Высокая                    & Высокая                      \\ \hline
			Обучения с подкреплением & $O(a)$                  & Высокая                    & Высокая                      \\ \hline
		\end{tabular}
	\end{adjustbox}
\end{table}

\subsubsection*{Вывод}

Исходя из сравнительной таблицы можно сделать следующие выводы:
\begin{enumerate}
	\item Наименее трудоемким для применения является метод обучения с подкреплением, однако он плохо интерпретируем и трудоемок при обучении.
	\item Метод потенциальных полей и теоретико-игровой методы обеспечивают высокую степень гибкости и правдоподобности движения агентов, однако трудоемкость работы алгоритмов, реализующих теоретико-игровой метод выше, чем у метода потенциальных полей.
\end{enumerate}
